{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristinoktvn/Data-Mining-4611-11271/blob/main/KNN_Gibran.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "ucgjIb_u4T8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bIpkF-psb2B",
        "outputId": "b02c5148-b545-46e3-fde1-94902aad669d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m581.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.1)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.23.5)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2023.6.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.3 ujson-5.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library Ekphrasis\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'date', 'number'],\n",
        "    annotate={\"hashtag\",\"allcaps\",\"elongated\",\"repeated\",'emphasis','censored'},\n",
        "    fix_html=True,\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmUVhhZR20hP",
        "outputId": "52396cee-af8c-4770-a70e-cd22d6678220"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "metadata": {
        "id": "oPNwIvQg24s8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset by Drive"
      ],
      "metadata": {
        "id": "SVrEJ8-F4nnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To Connecting GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uONOb-NB3Yf0",
        "outputId": "d76f1597-2f57-46f1-ef3b-c8fc286d25d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/FILE JOKI/M.ZIDNI ILMAN_A12.2019.06270/Bahan_Dataset/data fix.csv')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "IqS0K-jB3fsz",
        "outputId": "a27250a5-047a-49ea-83cf-98593a3f4b21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9a1b19605c82>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/FILE JOKI/M.ZIDNI ILMAN_A12.2019.06270/Bahan_Dataset/data fix.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FILE JOKI/M.ZIDNI ILMAN_A12.2019.06270/Bahan_Dataset/data fix.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the username column\n",
        "data = data.drop(['Username'], axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "c9YW3Is638Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the date column\n",
        "data = data.drop(['Date'], axis=1)\n",
        "data"
      ],
      "metadata": {
        "id": "SLcuIIvm4AQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Tweet"
      ],
      "metadata": {
        "id": "k5XCDCpi4FO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning Data\n",
        "#### Menghapus karatakter yang tidak diperlukan seperti hastag, tag, username, angka, link/url, dan elemen lainnya"
      ],
      "metadata": {
        "id": "FxvhQL0e5Srk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning data with a text processor from the Ekphrasis library\n",
        "def bersih_data(text):\n",
        "    return \" \".join(text_processor.pre_process_doc(text))\n",
        "\n",
        "# Replaces non-ASCII characters\n",
        "def non_ascii(text):\n",
        "    return text.encode('ascii', 'replace').decode('ascii')\n",
        "\n",
        "# Removes excess spaces\n",
        "def remove_space(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "# Removed excess emojis\n",
        "def remove_emoji(text):\n",
        "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\",\" \", text).split())\n",
        "\n",
        "# Removes special characters (\"\\t\", \"\\n\", \"\\u\")\n",
        "def remove_tab(text):\n",
        "    return text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
        "\n",
        "# Replace one or more space characters with a single space using regex\n",
        "def remove_tab2(text):\n",
        "    return re.sub('\\s+',' ',text)\n",
        "\n",
        "# Removed RT/Retweet terms\n",
        "def remove_rt(text):\n",
        "    return text.replace('RT',\" \")\n",
        "\n",
        "# Deleting mentions\n",
        "def remove_mention(text):\n",
        "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
        "\n",
        "# Deleting url\n",
        "def remove_incomplete_url(text):\n",
        "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
        "\n",
        "# Delete single characters\n",
        "def remove_single_char(text):\n",
        "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
        "\n",
        "# Replace dashes with spaces\n",
        "def change_stripe(text):\n",
        "    return text.replace('-',\" \")\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    remove = string.punctuation\n",
        "    remove = remove.replace(\"_\", \"\") # don't remove hyphens\n",
        "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
        "    return re.sub(pattern, \"\", text)"
      ],
      "metadata": {
        "id": "DHb2wl8w68bN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Folding\n",
        "#### Mengubah semua huruf menjadi lower atau huruf kecil"
      ],
      "metadata": {
        "id": "ORO50hVz9FT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lower(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "fEo0609M4CHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_string = []\n",
        "for text in data['Komen'].values:\n",
        "    EachReviewText = \"\"\n",
        "    proc = bersih_data(text)\n",
        "    proc = remove_rt(proc)\n",
        "    proc = lower(proc)\n",
        "    proc = change_stripe(proc)\n",
        "    proc = remove_tab(proc)\n",
        "    proc = remove_tab2(proc)\n",
        "    proc = non_ascii(proc)\n",
        "    proc = remove_incomplete_url(proc)\n",
        "    proc = remove_single_char(proc)\n",
        "    proc = remove_punctuation(proc)\n",
        "    proc = remove_space(proc)\n",
        "    EachReviewText = proc\n",
        "    final_string.append(EachReviewText)"
      ],
      "metadata": {
        "id": "xEq897BC5cTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the results of the data that has been cleaned\n",
        "data[\"step01\"] = final_string\n",
        "data"
      ],
      "metadata": {
        "id": "C0sc56-E5fEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View data information\n",
        "data.info()"
      ],
      "metadata": {
        "id": "u2S_lKDI5hDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n",
        "#### Memisahkan kata"
      ],
      "metadata": {
        "id": "UTD1HmT596eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_vOnmS865jRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "def word_tokenize_wrapper(text):\n",
        "  return word_tokenize(text)"
      ],
      "metadata": {
        "id": "XXv8hXFO5l3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the results of data that has been separated by words\n",
        "data['tokens'] = data['step01'].apply(word_tokenize_wrapper)\n",
        "data"
      ],
      "metadata": {
        "id": "y-X8FtX75l6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalisasi\n",
        "#### Penggantian kata-kata dalam teks berdasarkan kamus perbaikan kata"
      ],
      "metadata": {
        "id": "tML9AHUB-uDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calls the word repair dictionary file\n",
        "normalized_word = pd.read_excel(\"/content/drive/MyDrive/FILE JOKI/M.ZIDNI ILMAN_A12.2019.06270/Bahan_Dataset/kamus perbaikan kata.xlsx\")\n",
        "\n",
        "normalized_word_dict = {}\n",
        "\n",
        "for index, row in normalized_word.iterrows():\n",
        "    if row[0] not in normalized_word_dict:\n",
        "        normalized_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]\n",
        "\n",
        "data['tokens_perbaikan'] = data['tokens'].apply(normalized_term)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "5XJHBy_35pfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword Removal\n",
        "#### Mengapus kata yang bersifat umum seperti (aku, kamu, di, yang, dll)"
      ],
      "metadata": {
        "id": "7PpdRisc_FIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a collection of stopwords provided by NLTK\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "0txXSpjf5pil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans text from words identified as stopwords in Indonesian\n",
        "stopword = nltk.corpus.stopwords.words('indonesian')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  text = [word for word in text if word not in stopword]\n",
        "  return text\n",
        "\n",
        "data['Stop_removal'] = data['tokens_perbaikan'].apply(lambda x: remove_stopwords(x))\n",
        "data"
      ],
      "metadata": {
        "id": "APlFYptR5plL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts a list of each row in the 'Stop_removal' column to single text separated by spaces\n",
        "i=0\n",
        "final_string_tokens = []\n",
        "for text in data['Stop_removal'].values:\n",
        "    EachReviewText = \"\"\n",
        "    EachReviewText = ' '.join(text)\n",
        "    final_string_tokens.append(EachReviewText)"
      ],
      "metadata": {
        "id": "Bi4dJHJE5wlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View data results\n",
        "data[\"step02\"] = final_string_tokens\n",
        "data"
      ],
      "metadata": {
        "id": "opIWy4wb5wns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming\n",
        "#### Mengubah kata yang memiliki imbuhan menjadi kata dasar"
      ],
      "metadata": {
        "id": "ryo_EIKlAu7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Sastrawi Module\n",
        "!pip install sastrawi"
      ],
      "metadata": {
        "id": "QPROhUEN51dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the StemmerFactory module from Sastrawi which is a library for doing stemming in Indonesian\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "A225dHX851f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming Process\n",
        "final_string = []\n",
        "s = \"\"\n",
        "total_sentences = len(data[\"step02\"].values)  # Mengetahui total kalimat dalam dataset\n",
        "for idx, sentence in enumerate(data[\"step02\"].values, start=1):\n",
        "    print(f\"Melakukan stemming pada kalimat ke-{idx} dari total {total_sentences} kalimat\")\n",
        "\n",
        "    filteredSentence = []\n",
        "    EachReviewText = \"\"\n",
        "    s = (stemmer.stem(sentence))\n",
        "    filteredSentence.append(s)\n",
        "\n",
        "    EachReviewText = ' '.join(filteredSentence)\n",
        "    final_string.append(EachReviewText)\n",
        "\n",
        "print(\"Proses stemming selesai.\")"
      ],
      "metadata": {
        "id": "vdyg7VlC51iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View data results\n",
        "data[\"ProcessedText\"] = final_string\n",
        "data"
      ],
      "metadata": {
        "id": "jv5WQkR35wqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataframe in CSV format\n",
        "data.to_csv('hasilpreprocessing.csv',sep=\";\")"
      ],
      "metadata": {
        "id": "0C64ug37570Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pembobotan Kata\n",
        "#### Mengetahui seberapa sering muncul kata yang ada pada komentar"
      ],
      "metadata": {
        "id": "QeEDoeI_CmZx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OZfcqrwSDBPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementasi KNN"
      ],
      "metadata": {
        "id": "JuaoLxwkC-8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "j5Xymtzv572s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataframe that has previously been word cleaned\n",
        "dataset = pd.read_csv('hasilpreprocessing.csv',sep=\";\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "38NsDdLx574x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View dataset info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "snYaSbaG577Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensures that the content of the 'ProcessedText' column is considered a string data type\n",
        "dataset_feature = dataset['ProcessedText'].astype(str)\n",
        "dataset_feature"
      ],
      "metadata": {
        "id": "6XZKGrbk6b4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels or targets when performing machine learning tasks\n",
        "dataset_label = dataset['Label']\n",
        "dataset_label"
      ],
      "metadata": {
        "id": "-bh4JLXN6b-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Z80E_FoF579d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts the number of unique occurrences of a value in a column in a DataFrame\n",
        "dataset_label.value_counts()"
      ],
      "metadata": {
        "id": "w-jGGePQ6gnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visulisasi Data"
      ],
      "metadata": {
        "id": "oBZSofqTIUSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "value_counts = dataset['Label'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90, colors=['#66b3ff','#99ff99'])\n",
        "plt.title('Chart Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7drFFScpkEHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "aAPLMAYC6gpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing the dataset between training and testing\n",
        "train_x, test_x, train_label, test_label = train_test_split(dataset_feature, dataset_label, test_size = 0.1, random_state=0)"
      ],
      "metadata": {
        "id": "yLjOwr5A6gsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imported the TfidfVectorizer module from the scikit-learn library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Inisialisasi objek TfidfVectorizer\n",
        "Tfidf_Vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Perform transformation and formation of TF-IDF feature vectors from text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "iaVsxXWH6guk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displays the shape of the TF-IDF feature vector\n",
        "print(\"Shape of TF-IDF Matrix:\", tfidf_matrix.shape)\n",
        "\n",
        "# Returns a TF-IDF feature vector\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())"
      ],
      "metadata": {
        "id": "BUSOlgSsJq9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementasi KNN"
      ],
      "metadata": {
        "id": "Z8laiZIc6nxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "4SyDl4lt6gwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "classifier_knn = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('model', KNeighborsClassifier()),\n",
        "])"
      ],
      "metadata": {
        "id": "aRSRqeBw6tFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hasil = np.arange(1, 30, 2, dtype=int)\n",
        "hasil"
      ],
      "metadata": {
        "id": "vHbzrVho6tHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_knn = {\n",
        "               'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4),(2,3),(2,4),(3,4)],\n",
        "               'tfidf__use_idf': (True, False),\n",
        "               'model__n_neighbors': np.arange(1, 30, 2, dtype=int)\n",
        "}"
      ],
      "metadata": {
        "id": "uHPPd7MA6tK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_knn = GridSearchCV(classifier_knn, parameters_knn, cv = 3, n_jobs=-1)\n",
        "classifier_knn.fit(train_x, train_label.ravel())"
      ],
      "metadata": {
        "id": "XRp91hYE6tNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_knn_train = classifier_knn.predict(train_x)\n",
        "accuracy_knn_train = accuracy_score(train_label, y_pred_knn_train)\n",
        "print(\"Accuracy Training set: \", accuracy_knn_train)\n",
        "\n",
        "y_pred_knn_test = classifier_knn.predict(test_x)\n",
        "accuracy_knn_test = accuracy_score(test_label, y_pred_knn_test)\n",
        "print(\"Accuracy Test set: \", accuracy_knn_test)"
      ],
      "metadata": {
        "id": "2vF1gsBY6zxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_knn_train = recall_score(train_label, y_pred_knn_train, average='weighted')\n",
        "print(\"Recall Training set: \", recall_knn_train)\n",
        "\n",
        "recall_knn_test = recall_score(test_label, y_pred_knn_test, average='weighted')\n",
        "print(\"Recall Test set: \", recall_knn_test)"
      ],
      "metadata": {
        "id": "9AqoJy3H6zzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_knn_train = precision_score(train_label, y_pred_knn_train, average='weighted')\n",
        "print(\"Precision Training set: \", precision_knn_train)\n",
        "\n",
        "precision_knn_test = precision_score(test_label, y_pred_knn_test, average='weighted')\n",
        "print(\"Precision Test set: \", precision_knn_test)"
      ],
      "metadata": {
        "id": "-OVqh6956z1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_knn_train = f1_score(train_label, y_pred_knn_train, average='weighted')\n",
        "print(\"F1 Training set: \", f1_knn_train)\n",
        "\n",
        "f1_knn_test = f1_score(test_label, y_pred_knn_test, average='weighted')\n",
        "print(\"F1 Test set: \", f1_knn_test)"
      ],
      "metadata": {
        "id": "wQUH2vz66z4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "sns.heatmap(confusion_matrix(test_label, y_pred_knn_test), annot=True, cmap = 'viridis', fmt='.0f')\n",
        "plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
        "plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
        "plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wSw6EVPS6z6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_knn.best_estimator_"
      ],
      "metadata": {
        "id": "8aMa-ZQR68jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param_name in sorted(parameters_knn.keys()):\n",
        "    print(\"%s: %r\" % (param_name, classifier_knn.best_params_[param_name]))"
      ],
      "metadata": {
        "id": "E-ICV6js68l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordcloud"
      ],
      "metadata": {
        "id": "RQFo0Gm7kQJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "iv5fE9L8kOsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "c_jLyQaFjeag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#Word Cloud Teks\n",
        "f = open(\"/content/drive/MyDrive/FILE JOKI/M.ZIDNI ILMAN_A12.2019.06270/Bahan_Dataset/Text Wordcloud.txt\", \"r\")\n",
        "\n",
        "isi_text = f.read()"
      ],
      "metadata": {
        "id": "nI1z42RekZvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baca data\n",
        "print(isi_text)"
      ],
      "metadata": {
        "id": "uXLwtNYkkbrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width=1600, height=800, max_font_size=200, max_words=90, colormap='Set2', background_color='black')"
      ],
      "metadata": {
        "id": "m4jGAVmXkeOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud.generate(isi_text)"
      ],
      "metadata": {
        "id": "kF2rdjE1kewv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.savefig('wordcloud_positif.png')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ig9YSxrJkh27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_dictionary = wordcloud.process_text(isi_text)\n",
        "word_freq_pos = []\n",
        "\n",
        "word_freq_pos = {k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}\n",
        "\n",
        "print(list(word_freq_pos.items())[:10])"
      ],
      "metadata": {
        "id": "wY-XMyphkjx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}